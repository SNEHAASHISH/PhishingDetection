{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lM13ILweMRfd"
      },
      "source": [
        "#**Feature Extraction**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OzRmhC8NMbjM"
      },
      "source": [
        "#**Features are extracted from url dataset and these are catagorised into three catagories**\n",
        "# 1. Address Bar based Features\n",
        "# 2. Domain based Features\n",
        "# 3. HTML and Javascript based Features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmvyILspNiV7"
      },
      "source": [
        "#**Address Bar Based Features**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VmHK11CFQg_3",
        "outputId": "fcec0e6c-3cdf-4d86-f2ca-7473505cf362"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tldextract\n",
            "  Downloading tldextract-3.4.0-py3-none-any.whl (93 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.9/93.9 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.9/dist-packages (from tldextract) (3.11.0)\n",
            "Collecting requests-file>=1.4\n",
            "  Downloading requests_file-1.5.1-py2.py3-none-any.whl (3.7 kB)\n",
            "Requirement already satisfied: requests>=2.1.0 in /usr/local/lib/python3.9/dist-packages (from tldextract) (2.27.1)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.9/dist-packages (from tldextract) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests>=2.1.0->tldextract) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.1.0->tldextract) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.1.0->tldextract) (1.26.15)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from requests-file>=1.4->tldextract) (1.16.0)\n",
            "Installing collected packages: requests-file, tldextract\n",
            "Successfully installed requests-file-1.5.1 tldextract-3.4.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting python-whois\n",
            "  Downloading python-whois-0.8.0.tar.gz (109 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.6/109.6 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.9/dist-packages (from python-whois) (0.18.3)\n",
            "Building wheels for collected packages: python-whois\n",
            "  Building wheel for python-whois (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-whois: filename=python_whois-0.8.0-py3-none-any.whl size=103262 sha256=fa7ae88c811202f221389c10710f2bad95287f82d95a2cfc3aee2df8ab3b66ba\n",
            "  Stored in directory: /root/.cache/pip/wheels/e6/e9/d3/1e41a6c95b398de12c5a332ff28805aa44e68aa317ea60266d\n",
            "Successfully built python-whois\n",
            "Installing collected packages: python-whois\n",
            "Successfully installed python-whois-0.8.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.9/dist-packages (1.26.15)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting requests_html\n",
            "  Downloading requests_html-0.10.0-py3-none-any.whl (13 kB)\n",
            "Collecting w3lib\n",
            "  Downloading w3lib-2.1.1-py3-none-any.whl (21 kB)\n",
            "Collecting pyquery\n",
            "  Downloading pyquery-2.0.0-py3-none-any.whl (22 kB)\n",
            "Collecting pyppeteer>=0.0.14\n",
            "  Downloading pyppeteer-1.0.2-py3-none-any.whl (83 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.4/83.4 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from requests_html) (2.27.1)\n",
            "Collecting bs4\n",
            "  Downloading bs4-0.0.1.tar.gz (1.1 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting parse\n",
            "  Downloading parse-1.19.0.tar.gz (30 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting fake-useragent\n",
            "  Downloading fake_useragent-1.1.3-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.5/50.5 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi>=2021 in /usr/local/lib/python3.9/dist-packages (from pyppeteer>=0.0.14->requests_html) (2022.12.7)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.42.1 in /usr/local/lib/python3.9/dist-packages (from pyppeteer>=0.0.14->requests_html) (4.65.0)\n",
            "Collecting websockets<11.0,>=10.0\n",
            "  Downloading websockets-10.4-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (106 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.5/106.5 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3<2.0.0,>=1.25.8 in /usr/local/lib/python3.9/dist-packages (from pyppeteer>=0.0.14->requests_html) (1.26.15)\n",
            "Requirement already satisfied: importlib-metadata>=1.4 in /usr/local/lib/python3.9/dist-packages (from pyppeteer>=0.0.14->requests_html) (6.4.1)\n",
            "Requirement already satisfied: appdirs<2.0.0,>=1.4.3 in /usr/local/lib/python3.9/dist-packages (from pyppeteer>=0.0.14->requests_html) (1.4.4)\n",
            "Collecting pyee<9.0.0,>=8.1.0\n",
            "  Downloading pyee-8.2.2-py2.py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.9/dist-packages (from bs4->requests_html) (4.11.2)\n",
            "Requirement already satisfied: importlib-resources>=5.0 in /usr/local/lib/python3.9/dist-packages (from fake-useragent->requests_html) (5.12.0)\n",
            "Collecting cssselect>=1.2.0\n",
            "  Downloading cssselect-1.2.0-py2.py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: lxml>=2.1 in /usr/local/lib/python3.9/dist-packages (from pyquery->requests_html) (4.9.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->requests_html) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->requests_html) (2.0.12)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=1.4->pyppeteer>=0.0.14->requests_html) (3.15.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.9/dist-packages (from beautifulsoup4->bs4->requests_html) (2.4.1)\n",
            "Building wheels for collected packages: bs4, parse\n",
            "  Building wheel for bs4 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bs4: filename=bs4-0.0.1-py3-none-any.whl size=1270 sha256=54d19142d38bb7a222cabacfb621221e3fd4853a2272c9e9de442fb7cc21b890\n",
            "  Stored in directory: /root/.cache/pip/wheels/73/2b/cb/099980278a0c9a3e57ff1a89875ec07bfa0b6fcbebb9a8cad3\n",
            "  Building wheel for parse (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for parse: filename=parse-1.19.0-py3-none-any.whl size=24589 sha256=eaeee17b4ce6dced31c2b9b7e2f779df37853a17a4c800bf66acb29363abd925\n",
            "  Stored in directory: /root/.cache/pip/wheels/d6/9c/58/ee3ba36897e890f3ad81e9b730791a153fce20caa4a8a474df\n",
            "Successfully built bs4 parse\n",
            "Installing collected packages: pyee, parse, websockets, w3lib, cssselect, pyquery, pyppeteer, fake-useragent, bs4, requests_html\n",
            "Successfully installed bs4-0.0.1 cssselect-1.2.0 fake-useragent-1.1.3 parse-1.19.0 pyee-8.2.2 pyppeteer-1.0.2 pyquery-2.0.0 requests_html-0.10.0 w3lib-2.1.1 websockets-10.4\n"
          ]
        }
      ],
      "source": [
        "#download required module \n",
        "!pip install tldextract\n",
        "!pip install python-whois\n",
        "!pip install urllib3\n",
        "!pip install requests_html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M0ZNoj4bNolw"
      },
      "outputs": [],
      "source": [
        "from urllib.parse import urlparse,urlencode\n",
        "import ipaddress\n",
        "import re\n",
        "import tldextract\n",
        "from bs4 import BeautifulSoup\n",
        "import whois\n",
        "import urllib\n",
        "import urllib.request\n",
        "from datetime import datetime\n",
        "import requests\n",
        "import urllib3\n",
        "import json \n",
        "from urllib.parse import urlparse\n",
        "import ssl, socket\n",
        "from requests_html import HTML\n",
        "from requests_html import HTMLSession\n",
        "from urllib.parse import urlparse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-mjpjZoyIaJs"
      },
      "outputs": [],
      "source": [
        "fhd=['Weebly','DuckDNS','000webhost','Blogspot','Wix','GoogleSites','Github','Firebase','Squareup',\n",
        "     'ZohoForms','Wordpress','GoogleForms','Sharepoint','Yolasite','MyFTP','GoDaddysites','Mailchimp',\n",
        "     'Atwebpages','glitch','Webnode','Herokuapp','website','Netlify','hPage','InfinityFree',\n",
        "     'ByetHost','HyperPHP','AwardSpace','Freehostia','FreeHosting','FreeWebHostingArea','HostPapa','Ultahost',\n",
        "     'Porkbun','Bluehost','GoogieHost','x10hosting','Freehosting','Freehostia','SiteGround','DreamHost',\n",
        "     'HostGator','WordPress','DomainRacer','FreeHostingNoAds','FreeWebHostingArea','Namecheap',\n",
        "     'InMotionHosting','A2Hosting','interserver']\n",
        "for i in range(len(fhd)):\n",
        "    fhd[i]=fhd[i].lower()\n",
        "fhd=set(fhd)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zk_9LNCuN__N"
      },
      "source": [
        "### **IP Address in the URL**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IgLQXvNnOE5o"
      },
      "outputs": [],
      "source": [
        "# # Checks for IP address in URL (Have_IP)\n",
        "# def havingIP(url):\n",
        "#   try:\n",
        "#     ipaddress.ip_address(url)\n",
        "#     ip = 1          # phishing\n",
        "#   except:\n",
        "#     ip = 0          # legitimate\n",
        "#   return ip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WsOevqvZNmy-"
      },
      "source": [
        "##**HTTPS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Af3ZRo51NphT"
      },
      "outputs": [],
      "source": [
        "#checks that scheme of url is https/http\n",
        "def httpsScheme(url):\n",
        "    a = urlparse(url)\n",
        "    if a.scheme=='https':\n",
        "        return 0\n",
        "    return 1\n",
        "# httpsScheme('https://help.crowdtangle.com/en/articles/4201940-about-us')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5rArfQrOdQq"
      },
      "source": [
        "###**\"@\" Symbol in URL**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BGaH9L8TOe-J"
      },
      "outputs": [],
      "source": [
        "# Checks the presence of @ in URL (Have_At)\n",
        "def haveAtSign(url):\n",
        "  if \"@\" in url:\n",
        "    at = 1        # phishing\n",
        "  else:\n",
        "    at = 0        # legitimate\n",
        "  return at\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFU07t3ZOlpj"
      },
      "source": [
        "###**Length of URL**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p2uGk3V6OndD"
      },
      "outputs": [],
      "source": [
        "# Finding the length of URL and categorizing (URL_Length)\n",
        "def getLength(url):\n",
        "    return len(url)\n",
        "#   if len(url) < 54:\n",
        "#     length = 0      # legitimate      \n",
        "#   else:\n",
        "#     length = 1      # phishing   \n",
        "#   return length\n",
        "# getLength('https://siporados15585.blogspot.com/')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8I405VqOvnr"
      },
      "source": [
        "###**Depth of URL**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EndG1QcDOxpo"
      },
      "outputs": [],
      "source": [
        "# Gives number of '/' in URL (URL_Depth)\n",
        "def getDepth(url):\n",
        "  s = urlparse(url).path.split('/')\n",
        "  depth = 0\n",
        "  for j in range(len(s)):\n",
        "    if len(s[j]) != 0:\n",
        "      depth = depth+1\n",
        "  return depth\n",
        "#getDepth('https://app-br-portal-delicias.blogspot.com/?gclid=EAIaIQobChMIzKHVgO_r-wIVBY7ICh3KuQnwEAAYASAAEgL4xvD_BwE')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MyOGLy0IO7Kr"
      },
      "source": [
        "###**Redirection \"//\" in URL**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NN1VCN2MPH32"
      },
      "source": [
        "###**\"http/https\" in Domain name**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YXY5tNgAO3zT"
      },
      "outputs": [],
      "source": [
        "# Checking for redirection '//' in the url (Redirection)\n",
        "def redirection(url):\n",
        "  pos = url.rfind('//')\n",
        "  if pos > 6:\n",
        "    if pos > 7:\n",
        "      return 1       # phishing\n",
        "    else:\n",
        "      return 0       # legitimate\n",
        "  else:\n",
        "    return 0         # legitimate\n",
        "# redirection('http://www.legitimate.com//http://www.phishing.com')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lcwoAGP3PDtE"
      },
      "outputs": [],
      "source": [
        "# Existence of \"HTTPS\" Token in the Domain Part of the URL (https_Domain)\n",
        "def httpDomain(url):\n",
        "  domain = urlparse(url).netloc\n",
        "  if 'https' in domain:\n",
        "    return 1       # phishing\n",
        "  elif 'http' in domain:\n",
        "      return 1     # phishing\n",
        "  else:\n",
        "    return 0       # legitimate\n",
        "#httpDomain('http://httpeugnerally-wixsite-com.filesusr.com/html/1b57a2_7b8fe51ab2e0ec376d9dc985ff18e91c.html')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgGCDI3_PMtT"
      },
      "source": [
        "###**Using URL Shortening Services \"TinyURL\"**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZKwlIevRPOrQ"
      },
      "outputs": [],
      "source": [
        "# # listing shortening services\n",
        "# short_url_service =   r\"po\\.st|bc\\.vc|twitthis\\.com|u\\.to|j\\.mp|buzurl\\.com|cutt\\.us|u\\.bb|yourls\\.org|x\\.co|\" \\\n",
        "#                       r\"doiop\\.com|short\\.ie|kl\\.am|wp\\.me|rubyurl\\.com|om\\.ly|to\\.ly|bit\\.do|t\\.co|lnkd\\.in|db\\.tt|\" \\\n",
        "#                       r\"yfrog\\.com|migre\\.me|ff\\.im|tiny\\.cc|url4\\.eu|twit\\.ac|su\\.pr|twurl\\.nl|snipurl\\.com|\" \\\n",
        "#                       r\"prettylinkpro\\.com|scrnch\\.me|filoops\\.info|vzturl\\.com|qr\\.net|1url\\.com|tweez\\.me|v\\.gd|\" \\\n",
        "#                       r\"short\\.to|BudURL\\.com|ping\\.fm|post\\.ly|Just\\.as|bkite\\.com|snipr\\.com|fic\\.kr|loopt\\.us|\" \\\n",
        "#                       r\"qr\\.ae|adf\\.ly|goo\\.gl|bitly\\.com|cur\\.lv|tinyurl\\.com|ow\\.ly|bit\\.ly|ity\\.im|q\\.gs|is\\.gd|\" \\\n",
        "#                       r\"bit\\.ly|goo\\.gl|shorte\\.st|go2l\\.ink|x\\.co|ow\\.ly|t\\.co|tinyurl|tr\\.im|is\\.gd|cli\\.gs|\" \\\n",
        "#                       r\"tr\\.im|link\\.zip\\.net\"\n",
        "# # Checking for Shortening Services in URL (Tiny_URL)\n",
        "# def tinyURL(url):\n",
        "#     match=re.search(short_url_service,url)\n",
        "#     if match:\n",
        "#         return 1     # phishing\n",
        "#     else:\n",
        "#         return 0     # legitimate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27zDWieJP1kV"
      },
      "source": [
        "###**Prefix or Suffix \"-\" in Domain**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JYe9UumvP3I1"
      },
      "outputs": [],
      "source": [
        "# Checking for Prefix or Suffix Separated by (-) in the Domain (Prefix/Suffix)\n",
        "def prefixSuffix(url):\n",
        "    if '-' in urlparse(url).netloc:\n",
        "        return 1            # phishing\n",
        "    else:\n",
        "        return 0            # legitimate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cD5pP4jXQWeZ"
      },
      "source": [
        "###**contain subDomain or multiSubDomain**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hPLtcMI6QYJ2"
      },
      "outputs": [],
      "source": [
        "# Total number of dots in subdomain\n",
        "def subDomainCount(url):\n",
        "    result=tldextract.extract(url)\n",
        "    #print('https://'+(result.subdomain)+(result.domain)+(result.suffix))\n",
        "    subdomain=result.subdomain\n",
        "    if len(subdomain.split('.'))>2:\n",
        "        return 1  # phishing\n",
        "    else:\n",
        "        return 0  # legitimate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jo_3yYVDStrl"
      },
      "source": [
        "#**Domain Based Features**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Vsqcu2GThxi"
      },
      "source": [
        "##**DNS Record**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LpmsGZ9HTmJh"
      },
      "outputs": [],
      "source": [
        "# DNS Record availability (DNS_Record)\n",
        "def dnsRecord(url):\n",
        "    dns = 0           # legitimate\n",
        "    try:\n",
        "        domain_name = whois.whois(urlparse(url).netloc)\n",
        "    except:\n",
        "        dns = 1       # phishing\n",
        "    return dns\n",
        "# dnsRecord('https://hty1iiu.firebaseapp.com/')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMQh_mNzUxPe"
      },
      "source": [
        "##**Age of Domain**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qd1-Ayn-Usqe"
      },
      "outputs": [],
      "source": [
        "# The difference between termination time and creation time is (Domain_Age)  \n",
        "def domainAge(url):\n",
        "    try:\n",
        "        domain_name = whois.whois(urlparse(url).netloc)\n",
        "        # print(domain_name)\n",
        "        creation_date = domain_name.creation_date\n",
        "        expiration_date = domain_name.expiration_date\n",
        "        if (isinstance(creation_date,str) or isinstance(expiration_date,str)):\n",
        "            try:\n",
        "                creation_date = datetime.strptime(creation_date,'%Y-%m-%d')\n",
        "                expiration_date = datetime.strptime(expiration_date,\"%Y-%m-%d\")\n",
        "            except:\n",
        "                return 1   # phishing\n",
        "        if type(creation_date) is not list:\n",
        "            creation_date=[creation_date]\n",
        "        if type(expiration_date) is not list:\n",
        "            expiration_date=[expiration_date]\n",
        "        # print(creation_date)\n",
        "        # print(expiration_date)\n",
        "        if ((expiration_date is None) or (creation_date is None)):\n",
        "            return 1   # phishing\n",
        "        else:\n",
        "            ageofdomain = abs((expiration_date[0] - creation_date[0]).days)\n",
        "            if ((ageofdomain/30) < 6):\n",
        "                age = 1    # phishing\n",
        "            else:\n",
        "                age = 0    # legitimate\n",
        "        return age\n",
        "    except:\n",
        "        return 1 \n",
        "# domainAge('https://castanimex.blogspot.com/')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QquCvvxjZ_h0"
      },
      "source": [
        "##**End Period of Domain**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HkZxNaZKaF2f"
      },
      "outputs": [],
      "source": [
        "# The difference between termination time and current time is (Domain_End) \n",
        "def domainEnd(url):\n",
        "    try:\n",
        "        domain_name = whois.whois(urlparse(url).netloc)\n",
        "        # print(domain_name)\n",
        "        expiration_date = domain_name.expiration_date\n",
        "\n",
        "        if isinstance(expiration_date,str):\n",
        "            try:\n",
        "                expiration_date = datetime.strptime(expiration_date,\"%Y-%m-%d\")\n",
        "            except:\n",
        "                return 1  # phishing\n",
        "        if type(expiration_date) is not list:\n",
        "            expiration_date=[expiration_date]\n",
        "        if (expiration_date is None):\n",
        "            return 1  # phishing\n",
        "        else:\n",
        "            today = datetime.now()\n",
        "            end = abs((expiration_date[0] - today).days)\n",
        "            # print(expiration_date)\n",
        "            if ((end/30) > 6):\n",
        "                end = 0  # legitimate\n",
        "            else:\n",
        "                end = 1  # phishing\n",
        "        return end\n",
        "    except:\n",
        "        return 1 \n",
        "# domainEnd('https://att-yahoo-mail-104909.weeblysite.com/')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tN7_fj29cAYJ"
      },
      "source": [
        "#**HTML and Javascript based Features**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Qrx0CBPcKQ9"
      },
      "source": [
        "##**IFrame Redirection**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5jqHaV1qcCTg"
      },
      "outputs": [],
      "source": [
        "# IFrame Redirection (iFrame)\n",
        "def iFrame(soup):\n",
        "    web=soup.find_all('iframe')\n",
        "    if len(web)==0:\n",
        "        return 0\n",
        "    else:\n",
        "        for link in web:\n",
        "            if (str(link.get('src')).startswith(('https://','http://','/'))):\n",
        "                return 1\n",
        "        else:\n",
        "            return 0 \n",
        "# response=requests.get('https://www.cricbuzz.com/live-cricket-scores/66197/csk-vs-lsg-6th-match-indian-premier-league-2023')\n",
        "# iFrame(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRq8FjWadS_C"
      },
      "source": [
        "##**Status Bar Customization**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Td4pixUpdVcj"
      },
      "outputs": [],
      "source": [
        "# Checks the effect of mouse over on status bar (Mouse_Over)\n",
        "def mouseOver(response): \n",
        "    mouseaction=re.findall('onmouseover=\"([^ >\"]*)\"', response.text)\n",
        "    # print(mouseaction)\n",
        "    for ac in mouseaction:\n",
        "        if 'status' in ac:\n",
        "            return 1   # phishing\n",
        "    else:\n",
        "      return 0  # legitimate\n",
        "# response=requests.get('https://smartfreshtoday.blogspot.com/')\n",
        "# mouseOver(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WcyYbt_Lo6QT"
      },
      "source": [
        "##**Disabling Right Click**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TMq-ROoso8bw"
      },
      "outputs": [],
      "source": [
        "# Checks the status of the right click attribute (Right_Click)\n",
        "def rightClick(soup):\n",
        "    # soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    web=soup.find_all('body')\n",
        "    for link in web:\n",
        "        if link.get('oncontextmenu')=='return false':\n",
        "            return 1\n",
        "    web=soup.find_all('script')\n",
        "    for item in web:\n",
        "        if 'preventDefault()' in item:\n",
        "            return 1 \n",
        "    return 0\n",
        "# response=requests.get('https://leetcode.com/')\n",
        "# rightClick(response)\n",
        "# document.addEventListener('contextmenu', event => event.preventDefault());\n",
        "# oncontextmenu=\"return false\"\n",
        "#   $(function() {\n",
        "#         $(this).bind(\"contextmenu\", function(e) {\n",
        "#             e.preventDefault();\n",
        "#         });\n",
        "#     }); "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucnrdM4o8q2w"
      },
      "source": [
        "##**Website Forwarding**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aWbmVk3-8v8F"
      },
      "outputs": [],
      "source": [
        "# Checks the number of forwardings (Web_Forwards)    \n",
        "def forwarding(response):\n",
        "    if len(response.history) <= 2:\n",
        "        return 0 # legitimate\n",
        "    else:\n",
        "        return 1  # phishing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "poGgVijWJVtv"
      },
      "source": [
        "##**Google Index**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6lcB9S2ZJYXX"
      },
      "outputs": [],
      "source": [
        "def googleIndex(url):\n",
        "    google = \"https://www.google.com/search?q=site:\" + url + \"&hl=en\"\n",
        "    try:\n",
        "        response = requests.get(google, cookies={\"CONSENT\": \"YES+1\"})\n",
        "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "        not_indexed = re.compile(\"did not match any documents\")\n",
        "        if soup(text=not_indexed):\n",
        "            return 1  # phishing\n",
        "        else:\n",
        "            return 0  \n",
        "    except:\n",
        "        return 1\n",
        "# googleIndex('https://rameshwar.wordpress.com/')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sO4pbI-BJoup"
      },
      "source": [
        "##**Favicon**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3w59GsVeJql9"
      },
      "outputs": [],
      "source": [
        "# Checks for the favicon is loaded from internal source or external domain\n",
        "def validFavicon(soup,url):\n",
        "    result=tldextract.extract(url)\n",
        "    if len(result.subdomain)>0:\n",
        "        website_link='https://'+(result.subdomain)+'.'+(result.domain)+'.'+(result.suffix)\n",
        "    else:\n",
        "        website_link='https://'+(result.domain)+'.'+(result.suffix)\n",
        "    back_slash_link=website_link \n",
        "    if website_link[len(website_link)-1]!='/':\n",
        "        back_slash_link+='/'\n",
        "    icon_link = soup.find(\"link\", rel={\"icon\",\"shortcut icon\"})\n",
        "    if icon_link is None:\n",
        "        return 0\n",
        "    if str(icon_link[\"href\"]).startswith('/') or str(icon_link[\"href\"]).startswith(back_slash_link):\n",
        "        return 0  # legitimate\n",
        "    else:\n",
        "        return 1  # phishing\n",
        "# print(validFavicon('https://att-yahoo-101310.weeblysite.com/'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vO6WtP3uLVBL"
      },
      "source": [
        "##**Copy-Right Symbol**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n8JklZDKLWnJ"
      },
      "outputs": [],
      "source": [
        "# Checks for the copy-right symbol\n",
        "def copyRight(soup):\n",
        "    temp = soup.prettify().encode('UTF-8')\n",
        "    if(b'\\xc2\\xa9' in temp):\n",
        "        return 0\n",
        "    else:\n",
        "        return 1\n",
        "# copyRight('https://help.crowdtangle.com/en/articles/4201940-about-us')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SzO_IfMHL14j"
      },
      "source": [
        "##**Year of Copy-Right symbol**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z37uLdc9L3xl"
      },
      "outputs": [],
      "source": [
        "# Checks for the copy-right year \n",
        "def copyRightYear(soup):\n",
        "    copyrightTexts=\"\"\n",
        "    for tag in soup.findAll(string=re.compile(r'Copyright')):\n",
        "        copyrightTexts = tag.parent.text\n",
        "    result=re.findall(r'\\d+',copyrightTexts)\n",
        "    # print(result)\n",
        "    if(len(result))>0:\n",
        "        if int(result[len(result)-1])==2023:\n",
        "            return 0\n",
        "        else:\n",
        "            return 1\n",
        "    else:\n",
        "        return 1\n",
        "# copyRightYear('https://help.crowdtangle.com/en/articles/4201940-about-us')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_AwK_vXYMuXN"
      },
      "source": [
        "##**DomainName around Copy-Right Symbol**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L71nTJf0Mv4p"
      },
      "outputs": [],
      "source": [
        "# checks if domain name is found around copyright symbol or not\n",
        "def domainnameCopyRight(soup,url):\n",
        "    result=tldextract.extract(url)\n",
        "    # print(result)\n",
        "    if len(result.subdomain)>0:\n",
        "        website_link=(result.subdomain)+'.'+(result.domain)\n",
        "    else:\n",
        "        website_link=(result.domain)\n",
        "    # print(website_link)\n",
        "    copyrightTexts=\"\"\n",
        "    for tag in soup.findAll(string=re.compile(r'Copyright')):\n",
        "        copyrightTexts = tag.parent.text\n",
        "    copyrightTexts=copyrightTexts.split()\n",
        "    for i in range(len(copyrightTexts)):\n",
        "        copyrightTexts[i]=copyrightTexts[i].lower()\n",
        "    copyrightTexts=set(copyrightTexts)\n",
        "    if website_link in copyrightTexts:\n",
        "        return 0\n",
        "    else:\n",
        "        return 1\n",
        "# domainnameCopyRight('https://codeforces.com/')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3p2s5eRANHko"
      },
      "source": [
        "##**Link Ratio**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hDaJ-r5BNI7O"
      },
      "outputs": [],
      "source": [
        "# checks for the link ratio(Ratio of number of links redirecting to domain,to the number of total hyperlinks in the page)\n",
        "def linkRatio(soup,website_link):\n",
        "    no_back_slash_link=website_link \n",
        "    if website_link[len(website_link)-1]=='/':\n",
        "        no_back_slash_link=website_link[0:len(website_link)-1]\n",
        "    try:\n",
        "        same,total=0.0,0.0\n",
        "        for link in soup.find_all(\"a\", href=True):\n",
        "            if str(link[\"href\"])==website_link or str(link[\"href\"])=='/' or str(link[\"href\"])==no_back_slash_link or str(link[\"href\"])==website_link+'/':\n",
        "                same+=1 \n",
        "            total+=1\n",
        "        return round(same/total,6)\n",
        "    except:\n",
        "        return 1\n",
        "# jitni kam hai utni better hai\n",
        "# r=requests.get('https://talktoind.wordpress.com')\n",
        "# soup=BeautifulSoup(r.text,'html.parser')\n",
        "# linkRatio(soup,'https://talktoind.wordpress.com/')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3owuucGGN3AB"
      },
      "source": [
        "##**SSL name same as domain name**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OPiPqCSQN4bk"
      },
      "outputs": [],
      "source": [
        "# checks SSL name same as domain name\n",
        "def checkSSLname(url):\n",
        "    try:\n",
        "        weblink=urlparse(url).netloc\n",
        "        hostname = weblink\n",
        "        ctx = ssl.create_default_context()\n",
        "        with ctx.wrap_socket(socket.socket(), server_hostname=hostname) as s:\n",
        "            s.connect((hostname, 443))\n",
        "            cert = s.getpeercert()\n",
        "        subject = dict(x[0] for x in cert['subject'])\n",
        "        issued_to = subject['commonName']\n",
        "        # issuer = dict(x[0] for x in cert['issuer'])\n",
        "        # print(issuer)\n",
        "        # issued_by = issuer['commonName']\n",
        "        if issued_to==weblink:\n",
        "            return 0 \n",
        "        else:\n",
        "            return 1\n",
        "    except:\n",
        "        return 1\n",
        "# checkSSLname('https://framer.website')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IO_wqZTpO_It"
      },
      "source": [
        "##**Submit From Handler**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RbyqAculPAtS"
      },
      "outputs": [],
      "source": [
        "#checks the submition of form to blank \n",
        "def SFH(html):\n",
        "    formhandle = re.findall('<form.*(?!>)action=\"([^ >\"]*)\".*>',html)\n",
        "    # print(formhandle)\n",
        "    for form in formhandle:\n",
        "        if form.startswith(\"about:blank\"):\n",
        "            return 1\n",
        "        elif form.startswith('http://') or form.startswith('https://') or form.startswith('/'):\n",
        "            return 0\n",
        "        else:\n",
        "            return 0\n",
        "    else:\n",
        "        return 0\n",
        "# SFH('https://codeforces.com/register')\n",
        "# SFH('https://leetcode.com/accounts/login/')\n",
        "# print(SFH('https://codeforces.com/register'))\n",
        "# print(SFH('https://leetcode.com/accounts/login/'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jA0MMcRQ3XO"
      },
      "source": [
        "##**FormMail**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5VfCM9b1Q40g"
      },
      "outputs": [],
      "source": [
        "#checks the submition of form to mail \n",
        "def FormMail(html):\n",
        "    formhandle = re.findall('<form.*(?!>)action=\"([^ >\"]*)\".*>',html)\n",
        "    for form in formhandle:\n",
        "        if 'mail(' in form or 'mailto:' in form:\n",
        "            return 1\n",
        "    else:\n",
        "        return 0 \n",
        "# print(FormMail('https://codeforces.com/register'))\n",
        "# print(FormMail('https://leetcode.com/accounts/login/'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r81kZpN7SXjr"
      },
      "source": [
        "##**non Standard port**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xyWhPrmiSZM6"
      },
      "outputs": [],
      "source": [
        "def domain_port_num(url):\n",
        "    weblink=urlparse(url)\n",
        "    m=re.search('(?:http.*://)?(?P<domain>[^:/ ]+).?(?P<port>[0-9]*).*',url)\n",
        "    port = m.group('port')\n",
        "    if weblink.scheme not in {'https','http'}:\n",
        "        return 1 \n",
        "    if(len(port)>0):\n",
        "        if int(port) not in {80,443}:\n",
        "            return 1 \n",
        "        else:\n",
        "            return 0 \n",
        "    else:\n",
        "        return 0\n",
        "#domain_port_num('https://www.geeksforgeeks.org/how-to-get-rank-of-page-in-google-search-results-using-beautifulsoup/')\n",
        "# domain_port_num('https://www.example.co.uk:443/blog/article/search')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INTHp-ANWKIS"
      },
      "source": [
        "##**Total number of Subpages**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eHSGh-n5WLlX"
      },
      "outputs": [],
      "source": [
        "#checks total number of subpages (continuous value)\n",
        "def get_subpage_links(soup,website_link):\n",
        "    internal_link={}\n",
        "    back_slash_link=website_link \n",
        "    if website_link[len(website_link)-1]!='/':\n",
        "        back_slash_link+='/'\n",
        "    for link in soup.find_all(\"a\", href=True):\n",
        "        if str(link[\"href\"]).startswith('/') or str(link[\"href\"]).startswith(back_slash_link):\n",
        "            if link[\"href\"] not in internal_link:\n",
        "                internal_link[link[\"href\"]]=1;\n",
        "            else:\n",
        "                internal_link[link[\"href\"]]+=1\n",
        "    return len(internal_link)\n",
        "# r=requests.get('https://appimage.github.io')\n",
        "# soup=BeautifulSoup(r.text,'html.parser')\n",
        "# get_subpage_links(soup,'https://appimage.github.io')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjUMR9n9XVPe"
      },
      "source": [
        "##**Count of total indexed pages of google**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pM40ds_EXXOh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40d6d4d0-074b-4a18-9d66-dae18078d039"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "374000"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "#checks total number of pages related to the given url indexed by google \n",
        "def count_indexed_pages(url):\n",
        "    try:\n",
        "        session = HTMLSession()\n",
        "        response = session.get(\"https://www.google.co.uk/search?q=site%3A\" + url)\n",
        "        string = response.html.find(\"#result-stats\", first=True).text\n",
        "        count = int(string.split(' ')[1].replace(',',''))\n",
        "        return count\n",
        "    except:\n",
        "        return 0 \n",
        "# count_indexed_pages('https://codeforces.com/')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9kwySA6fyM-"
      },
      "source": [
        "##**Meta,Link and Script Tags**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pkq_bpjofy2F"
      },
      "outputs": [],
      "source": [
        "def checkpathdomain(pathlist,domain):\n",
        "\tcount = 0\n",
        "\ttry:\n",
        "\t\tfor path in pathlist:\n",
        "\t\t\tif path.startswith('http://') or path.startswith('https://') or path.startswith('//'):\n",
        "\t\t\t\tcount += 1\n",
        "\t\t\telif path.startswith('.') or path.startswith('/'):\n",
        "\t\t\t\tcontinue\n",
        "\t\t\telse:\n",
        "\t\t\t\turldomain = path.split('/')[0]\n",
        "\t\t\t\tif urldomain == domain:\n",
        "\t\t\t\t\tcontinue\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\tcount += 1\n",
        "\t\treturn count\n",
        "\texcept Exception as e:\n",
        "\t\treturn len(pathlist)\n",
        "#Checks whether external domain exists inside Meta,Link and Script tags \n",
        "def msltags(soup,url):\n",
        "\ttry: \n",
        "\t\tmetaurllist = re.findall('<meta.(?!>)URL=(.)\".*>', str(soup))\n",
        "\t\tscripturllist = re.findall('<script.*(?!>)src=\"(.*)\">', str(soup))\n",
        "\t\tlinklist = re.findall('<link.*(?!>)href=\"(.*)\">',str(soup))\n",
        "\t\ttotal = len(metaurllist) + len(scripturllist) + len(linklist)\n",
        "\t\tcount = checkpathdomain(metaurllist,url) + checkpathdomain(scripturllist,url) +checkpathdomain(linklist,url)\n",
        "\t\treturn round(count/total,6)\n",
        "\texcept:\n",
        "\t\treturn 1\n",
        "# r=requests.get('https://www.google.com/')\n",
        "# soup=BeautifulSoup(r.text,'html.parser')\n",
        "# msltags(soup,'https:www.//google.com/')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7iUVNeJ8FbW5"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "##**Anchor of URL**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LCkQ1BVtFc7w"
      },
      "outputs": [],
      "source": [
        "# Anchor of URL checks whether the external LINKS(other domain) contained within a webpage.\n",
        "# it's a ratio\n",
        "def external_links_ratio(soup,website_link):\n",
        "    no_back_slash_link=website_link \n",
        "    if website_link[len(website_link)-1]=='/':\n",
        "        # back_slash_link+='/'\n",
        "        no_back_slash_link=website_link[0:len(website_link)-1]\n",
        "    external_links=0 \n",
        "    anchor_links=soup.find_all(\"a\",href=True)\n",
        "    # for item in (anchor_links):\n",
        "    #     print(item)\n",
        "    total=len(anchor_links)\n",
        "    if total==0:\n",
        "        return 0\n",
        "    else:\n",
        "        for link in anchor_links:\n",
        "            if str(link[\"href\"]).startswith('/') or str(link[\"href\"]).startswith(no_back_slash_link):\n",
        "                external_links+=1 \n",
        "        # print(external_links,total)      \n",
        "        return round((total-external_links)/total,6)\n",
        "# r=requests.get('https://castanimex.blogspot.com')\n",
        "# soup=BeautifulSoup(r.text,'html.parser')\n",
        "# external_links_ratio(soup,'https://castanimex.blogspot.com')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjmkc8aXCgrP"
      },
      "source": [
        "##**Request URL**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nlT4zTFbCet0"
      },
      "outputs": [],
      "source": [
        "# Request URL checks whether the external objects contained within a webpage such as images, videos and sounds are loaded from another domain.\n",
        "# it's a ratio\n",
        "def total_external_links_from_audio_video_image(soup,website_link):\n",
        "    no_back_slash_link=website_link \n",
        "    if website_link[len(website_link)-1]=='/':\n",
        "        # back_slash_link+='/'\n",
        "        no_back_slash_link=website_link[0:len(website_link)-1]\n",
        "    video_count,audio_count,image_count=0,0,0 \n",
        "    image_links=soup.find_all(\"img\",src=True)\n",
        "    audio_links=soup.find_all(\"audio\", src=True)\n",
        "    video_links=soup.find_all(\"video\", src=True)\n",
        "\n",
        "    # for item in (image_links):\n",
        "    #     print(item)\n",
        "\n",
        "    if len(image_links)+len(audio_links)+len(video_links)==0:\n",
        "        return 0 \n",
        "    else:\n",
        "        for link in image_links:\n",
        "            if str(link[\"src\"]).startswith('/') or str(link[\"src\"]).startswith(no_back_slash_link):\n",
        "                image_count+=1 \n",
        "        for link in audio_links:\n",
        "            if str(link[\"src\"]).startswith('/') or str(link[\"src\"]).startswith(no_back_slash_link):\n",
        "                audio_count+=1 \n",
        "        for link in video_links:\n",
        "            if str(link[\"src\"]).startswith('/') or str(link[\"src\"]).startswith(no_back_slash_link):\n",
        "                audio_count+=1 \n",
        "    return round((len(video_links)-video_count+len(audio_links)-audio_count+len(image_links)-image_count)/(len(image_links)+len(audio_links)+len(video_links)),6)\n",
        "# r=requests.get('https://hty1iiu.firebaseapp.com/')\n",
        "# soup=BeautifulSoup(r.text,'html.parser')\n",
        "# total_external_links_from_audio_video_image(soup,'https://hty1iiu.firebaseapp.com/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w8QkQ-9-kQEP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCEl0VIfPTVE"
      },
      "source": [
        "## **Empty Links Ratio**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ztAxEDeIPR24"
      },
      "outputs": [],
      "source": [
        "# Empty Links Ratio is ratio of empty links to total links contained within a webpage.\n",
        "# it's a ratio\n",
        "def empty_links_ratio(soup,website_link):\n",
        "    back_slash_link=website_link \n",
        "    if website_link[len(website_link)-1]!='/':\n",
        "        back_slash_link+='/'\n",
        "    empty_links=0 \n",
        "    anchor_links=soup.find_all(\"a\",href=True)\n",
        "    # for item in (anchor_links):\n",
        "    #     print(item)\n",
        "    total=len(anchor_links)\n",
        "    if total==0:\n",
        "        return 0\n",
        "    else:\n",
        "        for link in anchor_links:\n",
        "            if (str(link[\"href\"])==\"#\") or (str(link[\"href\"])==\"#content\") or (str(link[\"href\"])==\"#skip\") or (str(link[\"href\"])==\"javascript:void(0)\"):\n",
        "                empty_links+=1  \n",
        "        return round(empty_links/total,6)\n",
        "# r=requests.get('https://hty1iiu.firebaseapp.com/')\n",
        "# soup=BeautifulSoup(r.text,'html.parser')\n",
        "# empty_links_ratio(soup,'https://hty1iiu.firebaseapp.com/')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UvX_F1xYHjki"
      },
      "source": [
        "##**Visibility Mode : Hidden**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y61FSCeQHhwe"
      },
      "outputs": [],
      "source": [
        "def check(arr,string):\n",
        "    if arr[0] in string.lower() and arr[1] in string.lower() and arr[2] in string.lower():\n",
        "        return 1 \n",
        "    return 0\n",
        "def visibilityMode(response,url):\n",
        "    res=[]\n",
        "    s = urlparse(url).netloc\n",
        "    s=s.split('.')\n",
        "    domain_p='nothing'\n",
        "    for item in s:\n",
        "        if item in fhd:\n",
        "            domain_p=item \n",
        "            break \n",
        "    for item in response.iter_lines():\n",
        "        item=str(item, 'ISO-8859-1')\n",
        "        ss=item.strip()\n",
        "        # print(ss,1)\n",
        "        if check(['visibility','hidden',domain_p],ss) or check(['type','hidden',domain_p],ss) or check(['display','block',domain_p],ss):\n",
        "            return 1 \n",
        "    else:\n",
        "        return 0\n",
        "# response=requests.get('https://hty1iiu.firebaseapp.com/')\n",
        "# visibilityMode(response,'https://hty1iiu.firebaseapp.com/')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4k8S0z3GM5Rp"
      },
      "source": [
        "#**Computing the feature's value for url passed as argument**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q0P_VP16NGcu"
      },
      "outputs": [],
      "source": [
        "def featureExtraction(url,label):\n",
        "    features=[]\n",
        "    #   Address Bar based Features\n",
        "    features.append(url)\n",
        "    # features.append(havingIP(url))\n",
        "    features.append(httpsScheme(url))\n",
        "    features.append(haveAtSign(url))\n",
        "    features.append(getLength(url))\n",
        "    features.append(getDepth(url))\n",
        "    features.append(redirection(url))\n",
        "    features.append(httpDomain(url))\n",
        "    # features.append(tinyURL(url))\n",
        "    features.append(prefixSuffix(url))\n",
        "    features.append(subDomainCount(url))\n",
        "    #   Domain Based Features\n",
        "    features.append(dnsRecord(url))\n",
        "    features.append(domainAge(url))\n",
        "    features.append(domainEnd(url))\n",
        "\n",
        "    #   HTML and Javascript based Features\n",
        "    valid=0 \n",
        "    try:\n",
        "        response=requests.get(url,timeout=4)\n",
        "        valid=1\n",
        "    except:\n",
        "        response=None \n",
        "    soup=None \n",
        "    if valid and response.status_code==200:\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "\n",
        "    if soup is not None:\n",
        "        features.append(iFrame(soup))\n",
        "    else:\n",
        "        features.append(1)\n",
        "\n",
        "\n",
        "    if valid and  response.status_code==200:\n",
        "        features.append(mouseOver(response))\n",
        "    else:\n",
        "        features.append(1)\n",
        "\n",
        "\n",
        "    if soup is not None:\n",
        "        features.append(rightClick(soup))\n",
        "    else:\n",
        "        features.append(1)\n",
        "\n",
        "\n",
        "    if valid and  response.status_code==200:\n",
        "        features.append(forwarding(response))\n",
        "    else:\n",
        "        features.append(1)\n",
        "\n",
        "    features.append(googleIndex(url))\n",
        "\n",
        "\n",
        "    if soup is not None:\n",
        "        features.append(validFavicon(soup,url))\n",
        "    else:\n",
        "        features.append(1)\n",
        "    \n",
        "\n",
        "    if soup is not None:\n",
        "        features.append(copyRight(soup))\n",
        "    else:\n",
        "        features.append(1)\n",
        "\n",
        "\n",
        "    if soup is not None:\n",
        "        features.append(copyRightYear(soup))\n",
        "    else:\n",
        "        features.append(1)\n",
        "\n",
        "    if soup is not None:\n",
        "        features.append(domainnameCopyRight(soup,url))\n",
        "    else:\n",
        "        features.append(1)\n",
        "    \n",
        "    if soup is not None:\n",
        "        features.append(linkRatio(soup,url))\n",
        "    else:\n",
        "        features.append(1)\n",
        "\n",
        "\n",
        "    features.append(checkSSLname(url))\n",
        "\n",
        "    if valid and  response.status_code==200:\n",
        "        features.append(SFH(response.text))\n",
        "    else:\n",
        "        features.append(1)\n",
        "\n",
        "\n",
        "    if valid and  response.status_code==200:\n",
        "        features.append(FormMail(response.text))\n",
        "    else:\n",
        "        features.append(1)\n",
        "\n",
        "    features.append(domain_port_num(url))\n",
        "\n",
        "\n",
        "    if soup is not None:\n",
        "        features.append(get_subpage_links(soup,url))\n",
        "    else:\n",
        "        features.append(0)\n",
        "\n",
        "    features.append(count_indexed_pages(url))\n",
        "\n",
        "    if soup is not None:\n",
        "        features.append(msltags(soup,url))\n",
        "    else:\n",
        "        features.append(1)\n",
        "\n",
        "    if soup is not None:\n",
        "        features.append(external_links_ratio(soup,url))\n",
        "    else:\n",
        "        features.append(1)\n",
        "\n",
        "    if soup is not None:\n",
        "        features.append(total_external_links_from_audio_video_image(soup,url))\n",
        "    else:\n",
        "        features.append(1)\n",
        "\n",
        "    if soup is not None:\n",
        "        features.append(empty_links_ratio(soup,url))\n",
        "    else:\n",
        "        features.append(1)\n",
        "\n",
        "    if valid and  response.status_code==200:\n",
        "        features.append(visibilityMode(response,url))\n",
        "    else:\n",
        "        features.append(1)\n",
        "\n",
        "    features.append(label)\n",
        "    return features "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijAWPNoY7QXm"
      },
      "source": [
        "#**Creating Dataframe**\n",
        "##  sno    feature_names\n",
        "##  1         Domain\n",
        "##  2         Have_IP      (not included)       \n",
        "##  3         Have_HTTPS\n",
        "##  4         Have_At\n",
        "##  5         URL_Length\n",
        "##  6         URL_Depth\n",
        "##  7         Redirection\n",
        "##  8         http_https_Domain\n",
        "##  9         TinyURL      (not included)        \n",
        "##  10        Prefix_Suffix\n",
        "##  11        SubDomainCount\n",
        "##  12        DNS_Record\n",
        "##  13        AgeOfDomain\n",
        "##  14        Domain_End\n",
        "##  15        iFrame\n",
        "##  16        Mouse_Over\n",
        "##  17        Right_Click\n",
        "##  18        Web_Forwards\n",
        "##  19        Google_Index\n",
        "##  20        Favicon\n",
        "##  21        CopyRightSymbol\n",
        "##  22        CopyRightYear\n",
        "##  23        Domain_Around_CopyRight\n",
        "##  24        LinkRatio\n",
        "##  25        SSL_Domain_Name\n",
        "##  26        Submit_Handler\n",
        "##  27        FormMail\n",
        "##  28        NonStandardPort\n",
        "##  29        SubPagesCount\n",
        "##  30        IndexPagesCount\n",
        "##  31        MLSTagsRatio\n",
        "##  32        Anchor_URL\n",
        "##  33        RequestUrl\n",
        "##  34        Empty_Link\n",
        "##  35        visibilityMode\n",
        "##  36        Label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "noat1g-u7ZPe"
      },
      "outputs": [],
      "source": [
        "feature_names = ['Domain','Have_HTTPS','Have_At', 'URL_Length', 'URL_Depth',\n",
        "                 'Redirection', 'http_https_Domain','Prefix_Suffix','SubDomainCount',\n",
        "                 'DNS_Record', 'AgeOfDomain', 'Domain_End', 'iFrame', 'Mouse_Over','Right_Click',\n",
        "                 'Web_Forwards','Google_Index','Favicon','CopyRightSymbol','CopyRightYear',\n",
        "                 'Domain_Around_CopyRight', 'LinkRatio','SSL_Domain_Name','Submit_Handler','FormMail',\n",
        "                 'NonStandardPort','SubPagesCount','IndexPagesCount','MLSTagsRatio','Anchor_URL',\n",
        "                 'RequestUrl','Empty_Link','visibilityMode','Label']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uXEGoC7pkqOV"
      },
      "outputs": [],
      "source": [
        "phish_values=[]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AgZuaey5JtEO"
      },
      "source": [
        "#**Algorithm to calculate features values for each urls from csv files**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FXY8I8go7awK"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np \n",
        "import csv \n",
        "web=pd.read_csv(\"/content/sample_data/blank22222.csv\") \n",
        "web=web['url'].copy() \n",
        "count=1 \n",
        "for item in web: \n",
        "    phish_values.append(featureExtraction(item.strip(),1)) \n",
        "    print(count,end=\" \") \n",
        "    count+=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HizezGk9j3K3"
      },
      "outputs": [],
      "source": [
        "print(len(phish_values))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NHl5m-9iJkVE"
      },
      "source": [
        "##**Algorithm to write in csv file**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qqGRo9CIMeYg"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "fields = feature_names\n",
        "rows = phish_values\n",
        "with open('/content/sample_data/blank_dataset.csv', 'w') as f:\n",
        "    csv_writer = csv.writer(f)\n",
        "    csv_writer.writerow(fields)\n",
        "    csv_writer.writerows(rows)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}